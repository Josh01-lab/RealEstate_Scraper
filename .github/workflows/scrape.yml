name: nightly-scrape

on:
  schedule:
    - cron: "0 18 * * *"     # daily 18:00 UTC
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      # ---- runtime knobs (override portals.json if needed) ----
      MAX_PAGES: "50"                # start conservative; raise later
      RATE_LIMIT_DELAY: "1.2"
      SCRAPING_MODE: "playwright"
      LOG_LEVEL: "INFO"
      # ---- DB (must be set in repo Secrets) ----
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      # optional: used by supabase_client if you decide to switch
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ‚úÖ Install Playwright browsers + OS deps (the common failure)
      - name: Install Playwright
        uses: microsoft/playwright-github-action@v1

      # Optional: show environment summary for debugging
      - name: Debug env
        run: |
          python -V
          pip freeze | sed -n '1,120p'
          echo "ENV=$ENV"
          echo "SCRAPING_MODE=$SCRAPING_MODE MAX_PAGES=$MAX_PAGES RATE_LIMIT_DELAY=$RATE_LIMIT_DELAY"
          # Masked, but confirms presence
          test -n "${DATABASE_URL}" && echo "DATABASE_URL present"

      # üå± Discovery (URL collection)
      - name: Run discovery (lamudi_cebu)
        run: |
          python -m src.scripts.discovery --portal lamudi_cebu

      # üîé Details (listing pages)
      - name: Run details (reuse same run dir)
        run: |
          LAST_RUN=$(ls -1d scraper_output/run_* | tail -n1)
          python -m src.scripts.details --portal lamudi_cebu --run-dir "$LAST_RUN"

      # üóÑÔ∏è Load to Postgres (Supabase)
      - name: Load latest JSONL to Postgres
        run: |
          LAST_RUN=$(ls -1d scraper_output/run_* | tail -n1)
          FILE="$LAST_RUN/staged/lamudi_cebu_listings.jsonl"
          test -f "$FILE"
          python -m src.etl.load_to_postgres --input "$FILE" --table listings

      # üì¶ Always upload artifacts so we can see logs/HTML if it fails
      - name: Upload scraper artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper_output_${{ github.run_id }}
          path: |
            scraper_output/**
            */.log
            **/debug_last.html
            **/debug_last.png
          if-no-files-found: ignore
