name: Nightly Lamudi scrape

on:
  schedule:
    # 03:00 Asia/Manila == 19:00 UTC daily
    - cron: "0 19 * * *"
  workflow_dispatch: {}

permissions:
  contents: read

concurrency:
  group: scrape-nightly
  cancel-in-progress: false

env:
  # ---------- Scraper env caps ----------
  SCRAPING_MODE: playwright        # use playwright in CI
  RATE_LIMIT_DELAY: "1.0"          # seconds between requests/pages
  MAX_PAGES: "1"                   # only first page of results
  MAX_LISTINGS: "50"               # hard cap on discovered URLs
  PYTHONUNBUFFERED: "2"
  PIP_DISABLE_PIP_VERSION_CHECK: "1"

  # ---------- Supabase (set these in repo Settings > Secrets and variables > Actions) ----------
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }} # or SERVICE_ROLE if you prefer

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('*/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps (project)
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers (Chromium)
        run: |
          python -m playwright install --with-deps chromium

      - name: Run discovery (lamudi_cebu)
        run: |
          python -m src.runner.run_discovery --portal lamudi_cebu
          echo "dir=$(ls -1d scraper_output/run_* | tail -n1)" >> $GITHUB_OUTPUT
        id: discover

      - name: Show discovery summary
        run: |
          echo "Run dir: ${{ steps.discover.outputs.dir }}"
          ls -la "${{ steps.discover.outputs.dir }}/staged" || true
          head -n 5 "${{ steps.discover.outputs.dir }}/staged/lamudi_cebu_urls.jsonl" || true

      - name: Run details (lamudi_cebu)
        run: |
          python -m src.runner.run_details --portal lamudi_cebu --run-dir "${{ steps.discover.outputs.dir }}"
          echo "==== First few listings ===="
          test -f "${{ steps.discover.outputs.dir }}/staged/lamudi_cebu_listings.jsonl" && head -n 3 "${{ steps.discover.outputs.dir }}/staged/lamudi_cebu_listings.jsonl" || (echo "No listings file written"; exit 1)

      - name: Publish to Supabase
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        run: |
          python -m src.runner.write_portal_dump_to_supabase --portal lamudi_cebu --run-dir "${{ steps.discover.outputs.dir }}"

      - name: Sanity check (count + latest rows)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        run: |
          python - << 'PY'
          from src.db.supabase_client import get_client
          sb = get_client()
          res = sb.table("listings").select("*", count="exact").eq("source","lamudi_cebu").execute()
          print("Total lamudi_cebu rows:", res.count)
          rows = (sb.table("listings")
                    .select("url,listing_title,address,property_type,price_php,area_sqm,price_per_sqm,published_at,scraped_at")
                    .eq("source","lamudi_cebu")
                    .order("scraped_at", desc=True)
                    .limit(5).execute().data)
          for r in rows: print(r)
          PY
